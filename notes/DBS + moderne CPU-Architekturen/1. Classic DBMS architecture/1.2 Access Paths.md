assignment: B+-Tree

---
# Free Space Management

##### Bitmap

>[!idea] nibbles indicate fill status (free or used space) of page

- traditional approach
- approximates status -> unused space
- logarithmic scale/mix of both to utilize space

##### Using a FSI

- compute FSI entry
- scan FSI for matching entry
- insert data on page


---
# Slotted Pages
Storage and Retrieval of Data
-> programming assignment
## Layout of Slotted Pages

>[!int] layout TODO

##### Header

- **LSN**: for recovery
- **slotCount**: number of used slots
- **firstFreeSlot**: speed up location of free slots
- **dataStart**: lower end of data
- **freeSpace**: space that would be available after compactification

##### Slots

- **offset**: start of data item
- **length**: length of data item


##### Indirections/Slot implementation


## Records
= how to materialize tuples

1. naive serialization: $\mathcal{O}(n)$ in worst case
2. **split tuple**: fixed-size header + variable-size tail -> $\mathcal{O}(1)$
	- even better: reorder attributes by decreasing alignment -> better performance without padding

![[Pasted image 20240430143304.png]]

>[!info] alignment
>offset/address is a multiple of size

*NOTE: never ever misalign atomics lmao*
- misaligned store: memory bus is locked, therefore it works (very expensive tho)
- intel: every load is atomic, if crossing cash lines -> two loads

## `NULL` values

unknown/unspecified value outside of regular domain

>[!idea] use separate null bit
>- collect all null bits and store them together in bytes

not recommended: special value
- numbers are asymmetric anyways
- $x = 0x8000...0000$ is weird bc $-x =x$ and also $x< 0$

----
# Compression
how to store Unusually Large Data

>[!int] reduced data size improves throughput
>- decompression should be reasonably fast too

[Huffman tree](https://en.wikipedia.org/wiki/Huffman_coding) -> "horrible, don't do this" bc expensive decompression

>[!info] special-purpose and byte-size compression are cheap options

## Integer Compression

>[!idea] variable length encoding: number of bytes needed + data

![[Pasted image 20240430145430.png|300]]

*NOTE: length are stored differently to prevent misalignment*

still some cost: location of compressed attributes depend on preceding compressions
- lookup tuples per length byte [TODO: what]

## Dictionary Compression

>[!idea] store strings in dictionary, then refer to them via ID

| advantages                               | disadvantages                                                       |
| ---------------------------------------- | ------------------------------------------------------------------- |
| can reduce data size                     | sorted by length -> updates can become very expensive               |
| can be combined with integer compression | different dicts on different systems -> sometimes only used locally |
| factors out common string                | unhelpful if strings are similar but unique (e.g. URLs)             |
## FSST (Fast Static Symbol Table)
-> [@boncz2020fsst] 

>[!idea] look at common patterns/substrings in data

| advantages                                         | disadvantages          |
| -------------------------------------------------- | ---------------------- |
| allows for random access                           | how to find the table? |
| very fast decompression (GB/s, optimization focus) |                        |
| search possible on compressed data                 |                        |
## LZ77

[Wikipedia](https://en.wikipedia.org/wiki/LZ77_and_LZ78#LZ77)

----
# Long Records

>[!goal] how to store tuples larger than a table?

in SQL:
**BLOB**: binary large object
**CLOB**: character large object

>[!idea] store string separately from tuple
>adds indirection but makes processing easier

## Storing BLOBs

#### Tree

>[!int] B-Tree with offsets as search keys
>- offsets relative to previous split

| advantages               | disadvantages      |
| ------------------------ | ------------------ |
| fast, flexible, powerful | over-sophisticated |
#### Extent List

>[!int] tuple points to BLOB tuple, which contains a header and an extent list

![[Pasted image 20240430152705.png]]

| advantages | disadvantages                         |
| ---------- | ------------------------------------- |
| simple     | can only manipulate BLOB in one piece |
#### General Approach

>[!goal] users misuse BLOBs/CLOBs, most are short

approach:
1. BLOBs < TID: encoded in BLOB TID
2. BLOBs < page size: store in BLOB record
3. larger BLOBs: use full mechanism

---
# Index Structures

>[!idea] how to deal with indexed data

**point queries**: find tuples with a given value
**range queries**: find tuples within given value range

hashtables:
- "unpleasant and really bad"
- unknown size
- bad at growing, spikes in response time (rehash)
- cannot handle duplicates

## B-Trees

>[!goal] dominant data structure for external storage

see [[2.2 Search trees#B-Tree]]

>[!info] B⁺-Trees are "simply superior"
>reminder: inner nodes separators, data (keys + TIDs) only in leaf nodes (linekd!)
>- inner nodes are smaller -> larger fan-out, better performance
>- simplifies logic (separators don't have to be data values)

![[Pasted image 20240507141225.png]]
#### Implementation

inner node:
- **LSN**
- **upper**: right-most child (anything larger than largest separator)
- **count**
- **key/child**

leaf node:
- **LSN**
- **leaf node marker**
- **next**: next leaf node
- **count**
- **key/tid**

*NOTE: for variable key sizes (eg strings), implementation looks similar to slotted pages*

#### Lookup

```
current := root
loop:
	if current is leaf:
		locate entry on current page
		return
	find first entry >= search key
	if found:
		go to page
	else:
		go to upper
```

2 variants: return entry or position on leaf page

*NOTE: performance depends on `locate` and `search` -> binary search*

or [fancy shit](http://databasearchitects.blogspot.com/2015/09/)
#### Insert

```
lookup leaf page

if free space on leaf:
	insert entry
	return
else:
	split leaf into two
	insert entry
	max of left page becomes separater in parent
	while parent overflows:
		split upwards
```

#### Delete

```
current = lookup leaf page
remove entry from current

loop:
	if current page at least half full:
		return
	
	if neighboring page more than half full:
		rebalance
		update separator
		return
	else:
		merge neighboring pages
		remove separator
```

"neighboring page" here is restricted to siblings

simplification: accept under-full pages in delete. but performance may get worse

#### Range Scan

```
lookup start
go through pages (next pointer) until stop value
```

>[!info] clustered B-Tree
>subsequent leafs are consecutive on disk -> amazing
### Compound keys

>[!idea] compare lexicographically (aka 1. key, then 2. key, ...)

- attributes can be bound or not
- prefixes can be searched, suffixes not (e.g. first name in phone book) -> range
	- order matters in `create_index`

### Non-Unique Values

evil-shmevil

>[!idea] only index unique values
>index `(name, TID)` instead of just `name` (works bc prefix search)

- leaf nodes become a little bigger
- access in $\mathcal{O}(\log n)$

### Concurrent Access

>[!prob] problem: pages depend on each other

**lock coupling**: latch root, latch 1. level, release root, latch 2. level
-> pages can only be split when the parent is latched

SLINKY :3

works fine for lookup, but for insert:
#### Safe inner pages

>[!prob] we might have to split grandparents, but using lock coupling leads to deadlocks

>[!idea] safe inner pages: while going through tree, split nodes if they don´t have enough space for one more entry

but: variable-sized entries :c
#### Restart

**restart** strategy:
1. try insert with simple lock coupling
2. if it doesn't work: restart with all latches up to root

-> rarely happens so its ok(TM)
#### B-link Trees

>[!prob] lock coupling effectively locks many many pages. goal: lock only one

- lookups/insert: can be done via next pointers
- difficult to safely delete
#### Optimistic Lock Coupling

[Optimistic concurrency control](https://en.wikipedia.org/wiki/Optimistic_concurrency_control)

>[!int] more reads than writes, mostly only leaf nodes are modified

- traditionally: inner nodes have shared locks, leaf nodes exclusive
- but shared locks are still expensive!

- fake lock in a sense. don't hold any actual locks in inner nodes. has version number and "exclusive" bit
- release: clear exclusive flag, increment version number
- version number tells us whether there has been a modification. if so, restart!

>[!int] nodes in upper level rarely modified
### Bulkloading

repeated inserts: random I/O, pages touched multiple times

>[!idea] sort data before insert -> better inserts and locality

*NOTE: usually the whole thing is locked*

but even better:
#### Bulk creation (from scratch)

1. sort data
2. create leaf pages, save largest values in a temp file
3. build tree bottom-up, create inner pages using saved separators, until only one inner page left

-> compact, clustered :> 
#### Bulk update

1. sort data
2. merge data into existing trees
3. new chunks are started once a page would only have entries from original

-> rip clustering, but minimal I/O
#### Partitioned B-Trees

>[!idea] make mostly independent partitions (except for boundary node)
>- new artificial column in front
>- bulk loads get a new partition number
>- operations are done within partitions (temporarily!)
>- partitions are merged over time

deletion: additional flag ("anti-matter")

TODO: adv/dis
### Variable Length Records

- usually: keys are opaque (exception: single strings)
- separator choice affects fanout+ space

non-trivial :c

bulkloading case: consider window around separator, use smallest as actual separator

general case: when page overflows, pick shortest and (if tied) closest value (20%) in window around median

## Hash-based indexes

>[!info] related ramblings: [[Hashtable stuff]]

challenges:
- external storage
- less versatile
-> mainly for primary key indexes
### Extendible Hashing

>[!idea] share table entries -> growth without rehashing
>- avoid random I/O
>- chaining expensive on disk

- table entries point to buckets, multiple can point to the same one -> **buddy system**
- buckets are split when overflowing
- "depth": basically how many bits are used to determine bucket. table is doubled if depth cannot be increased
- chain when a max table size is reached (hopefully rarely)

| Advantages                             | Disadvantages                                 |
| -------------------------------------- | --------------------------------------------- |
| 2 page accesses per lookup             | table doubling is very expensive              |
| growth independent of existing buckets | large steps in space consumption              |
| no re-hashing                          | hash collisions have devastating consequences |
### Linear Hashing

>[!idea] use buckets and chain when they overflow
>- avoid exponential growth

- split only a subset of buckets on demand
	- $[1,k[$ are split, $[k, n]$ are not

![[Pasted image 20240514151611.png]]

```
slot = hash % n    // n is a power of 2
if slot <= k
	slot = hash % (2 * n)
```

challenge: space management, how to actually grow the array (incrementally)?
solution: add indirection array (physical separation but virtually it's still one big array)

| Advantages             | Disadvantages                                                                                    |
| ---------------------- | ------------------------------------------------------------------------------------------------ |
| avoid chunky growth    | some chains :/                                                                                   |
| amortization says yay  | page allocation: probably can't reserve enough space, then we run into other stuff while growing |
| linearly growing index |                                                                                                  |
### Multi-Level Extendible Hashing

>[!idea] tree of hash tables
>- real data often skewed

- next level uses next $k$ bits -> large fanout, additional levels where heavily used
- uniform data is now worst case
- for better space utilization: share inner pages between buddies

![[Pasted image 20240514151537.png]]

>[!info] "everything can be inferred from the bit structure"

| Advantages                | Disadvantages            |
| ------------------------- | ------------------------ |
| shallow tree/large fanout | additional page accesses |
| directory growth is a-ok  |                          |
## Bitmap Indexes

>[!int] unselective predicates are meh

for small attribute domains: **bitmap indexes**
- one bitmap for every attribute
- index intersections = bit operations

-> sparse and easily compressed (run-length encoding)

## Small Materialized Aggregates

>[!idea] older static chunks: pre-compute/cache some info

- scan aggregate before actually scanning chunk -> possibility to skip
- eagerly or lazily update aggregate

## Multi-Dimensional Indexing

>[!info] beware of curse of dimensionality
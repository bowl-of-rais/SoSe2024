>[!int] hashing basically makes order random, but sometimes (often) we like order

operations: lower bound, upper bound, next/previous, fancy shit like prefix lookup/min/max/top-k

# Arrays

## Binary Search

regular:

```cpp
while (lower != upper) {
	unsigned middle = lower + ((upper-lower)/2);
	unsigned v = data[middle];
	if (v==needle) {
		return middle;
	} else if (v<needle) {
		lower = middle+1;
	} else {
		upper = middle;
	}
} 
```

- branch misses :c
- convert conditional branches into data dependencies:
	- unpredictable branches < data dependencies < predictable branches

```cpp
while (auto_t half=n/2) {
	auto middle = lower+half;
	lower = ((*middle)<=needle) ? middle : lower;
	n -= half;
}
return ((*lower)==needle) ? lower : notFound;
```

*NOTE: `?` makes it a `cmov`*
## Interpolation Search

>[!int] for roughly linearly spread out data, we can guess positions

-> very sensitive to outliers
## Linear Search

- locality, does not need sorting, good for small arrays
- improvement with SIMD: compare with array of repeated search value
## Blocking Layout

>[!idea] SIMD + binary search?

![[blocking.svg]]
# Growing Arrays

>[!idea] instead of doubling + copying, store parts of array

![[Pasted image 20240516103637.png|400]]
##### Lookup

```cpp
int& index(int* dict[64], u64 i) {
	u64 ckId = 64 - __builtin_clzl(i); // count leading zeroes
	int* chunk = dict[ckId];
	u64 j = i - (1 << (ckId-1));
	return chunk[j];
}
```

# Comparison-based Search Trees

>[!int] binary search as a tree

**nodes**: key, value, 2 children

- rebalancing
- bad cache locality

## Skip List

>[!idea] sorted linked list + shortcuts

- similar to bst: access pattern/cache misses

![[Pasted image 20240516104456.png]]
# B-Tree

>[!idea] multiple keys per node, all at least half full (except root)

## Bâº-tree ("B-tree")

variant:
- inner nodes store pointers only
- chained leaf nodes -> simpler range scan

#### Motivation

simplest data structure for search, range scans etc: **sorted array**
- lookup: $log_{2}(n)$
- insert: $n$ :c

quicker insert: multiple smaller, sorted arrays of fixed size $b$
- within array: lookup/insert/delete constant time (bc $b$ fixed)
- how to find right array?

add **inner nodes** indicating where to go

![[Pasted image 20240502112918.png]]

height: $log_{b}(n)$ -> in practice: good to use pretty large $b$ (e.g. 100)
comparisons: $log_{2}(n)$

>[!int] like a more cache-efficient layout for binary search/tree

>[!goal] "range-tree"
>- non-overlapping ranges that are determined during splits/merges
>- separator keys are range bounds
#### Insert

- if node has space: just insert
- otherwise: split node

worst case: split all nodes in path -> $log_{b}(n) \cdot b \in \mathcal{O}(log(n))$
#### Range Tree

- leaf nodes have ranges

![[Pasted image 20240516095844.png]]
#### Logic

2 kinds of operations:
1. per-node operations: often, can be heavily optimized
2. structure-modification operations: more rare

-> split in implementation. see [[Assignment 3. B-Tree]]
#### Physical Node Layouts

- array of sorted pairs
- alternatives: 
	- separated payloads
	- unsorted leaves: faster insert, lookup a bit slower, have to sort on split
	- combination: keys in order + indirection to unsorted payloads
	- other data structure (trees) in node ðŸŒ³
#### Node Size

- minimum: cache line size
- other factors influencing locality: eg cache line prefetching (adjacent cache lines almost as good as a single one), DRWM rows (we want to avoid skipping rows)
- too big: cache misses
- optimum for lookup > optimum for insert (shift shit)
	- inner nodes have different access patterns compared to leaf nodes
	- idea: optimize leaves for inserts, inner nodes for lookup
# B-Trees with variable length data

>[!info] dynamic pointers to keys: bad cache locality, memory allocation, fragmentation ðŸ˜¢
## Slotted Pages

*NOTE: fixes size of node, but number of keys can vary*
### Layout

>[!idea] slots: store offset and length

![[Pasted image 20240516110923.png|300]]

-> logically sorted, physically unsorted
### Compaction

>[!goal] deletion leaves holes -> how to deal with wasted memory?

1. during insert/delete: difficult, expensive
2. keep track of wasted bytes, trigger compaction **if needed**
	- implementation: copy keys to new empty page
	- split automatically compacts

*NOTE: if values > page, dynamically allocate + point*

### Further Optimizations
#### Separator Choice

>[!int] lookup: keys have indirection -> cache misses

>[!idea] instead of picking median, pick shortest key (in window)
#### Suffix Truncation

>[!idea] keys in inner nodes only used for comparisons

e.g. `ABCDE` and `AXYZ` can be separated by `AX`
#### Prefix Truncation

>[!idea] extract common prefix between smallest + largest key, compare with prefix first

![[Pasted image 20240516112708.png|350]]
##### Problems

- insert can change prefixes
- also what if multiple nodes have the same prefix

>[!idea] use prefix of separators instead

**fence key**s: separators; immutable after page was created
- store on page or derive
#### Key head in slot

>[!idea] remove indirection by storing first 4 bytes of key in slot
>- better locality during comparisons

number of bytes: 4/8, also depends on what else is in slot

*NOTE: for comparing 4 bytes, integer comparison is also an option. beware of endianness tho*
#### Hints

>[!idea] extract fixed number of equi-distant heads, store consecutively

- check hints first, then do binary search on specific range
- ideally: hints array = 1 cache line

![[Pasted image 20240523102203.png]]

*NOTE: must be updated on insert, but insert still faster bc faster lookup*

>[!goal] prefix optimization is what allows heads/hints to discriminate for long common prefixes
# Deletion and Space Management in B-trees

-> rebalance/merge after deletion to avoid underfull nodes

**rebalancing**: move some entries from neighbor node. tricky bc new separator etc

also: finding neighboring nodes, propagating merges. common prefixes may become shorter and stuff might not fit into a page after all

alternatives:
- don't merge, only delete in leaf (bunch of empty nodes, tree grows taller)
- only delete completely empty nodes (what if nodes have only a few entries?)
- only merge
- only merge nodes with same parent
- only merge leaves
- never shrink tree height (never delete root)
### About Space

- theoretically: 50% fill factor -> what does this mean for variable sized data?
- sorted insert - yikes
	- idea: split where inserted...?
- [waves of misery](https://btw.informatik.uni-rostock.de/download/tagungsband/B2-2.pdf): average fill factor fluctuates for random data bc splits are correlated
### B\*-Tree

- 66% fill factor
- rebalance between 3 or more neighboring nodes, implementation nightmare
### XMerge

>[!idea] merge X neighboring nodes into X-1 nodes
>- keep track of (global?) fill factor, merge if it gets low

- merge comparable to page eviction?
# BERT

Paper: [@devlin2019bert]

- transformer encoder
- pre-training tasks: masked language modeling, next sentence prediction

>[!star] Use case: pre-train once, finetune many times

### Extensions

- RoBERTa: bigger lmao
- SentenceBERT: siamese architecture, triplet loss [@reimers2019sentencebert]
	- better embeddings for sentences
	- triplet loss: anchor, positive, negative sentences -> make dist between a and p smaller than between a and n
- DIstilBERT: surrogate model with fewer parameters, replicates behavior of big model

----
# GPT

>[!idea] idea: transformer-based decorder trained on language modeling
## GPT-2

- meta-learning
- provide task description w/ input sample

## GPT-3

Paper: [@brown2020language]

>[!idea] make GPT-2 larger

- motivation: general-purpose model, don't rely on task-specific datasets/fine-tuning
- humans are few-shot learners too
- training: learning rate goes down as model size increases

#### Applications

- GitHub Copilot, AI Dungeon, DALL-E
- in games: scripted vs story developing naturally
- actually comes down to temporal logic

#### Limitations

- repeating itself, hallucinations, common sense
- biases, not knowledge-grounded

#### Compared to BERT

- larger
- BERT still potentially better for some tasks: classification, filling gaps tasks, etc
to find the best: check huggingface lmao
# T5

Paper: [@raffel2023exploring]

- every NLP problem as a seq2seq model
- task-specific prefix
- same architecture like original Transformer

# RETRO

Paper: [@borgeaud2022retro]

problems: costly models, fixed once trained, sometimes data shouldn't be used

>[!idea] idea: externalize knowledge

- special encoder for document chunks -> store embeddings

![[Pasted image 20240419155106.png]]

**retrieval oriented block**: attention -> cross-attention -> feed-forward
- neighbors included in prompt

faster bc knowledge retrieval is outsourced. just like with humans!

# [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral)

>[!idea] idea: improve performance (in terms of efficiency)

[**Mixture of Experts**](https://huggingface.co/blog/moe): "router" chooses a network (= *expert*)
- sparse version: slimmer/fewer parameters

![[Pasted image 20240419160246.png]]
# Stable Diffusion

Paper: [@rombach2022highresolution]

**diffusion**: train to de-noise Gaussian in pixel space, conditioned on different modalities (e.g. text)

*note: [DALL-E](https://www.assemblyai.com/blog/how-dall-e-2-actually-works/)*

*recap: Autoencoder vs [Variational Autoencoders](https://en.wikipedia.org/wiki/Variational_autoencoder): unspecified latent space vs latent space to sample from. approximate distribution with ELBO (regularizer to make latent space Gaussian + reconstruction)*

*recap: U-Net - down-sample with convolutions, then upsample again*
here: combined with [attention](1.2 Attention and Transformers#Attention)

![[Pasted image 20240419161350.png]]

>[!int] intuition: conditioning
>"happy face": circle + eyes + face
>low-dim latent space has essence of higher-dim space without unnecessary details, so condition there

**CLIP**: pairs of images and captions -> two-part encoder, contrastive learning
### Diffusion for text

##### GENIE - Text generation with diffusion LMs: a pre-training approach with continuous paragraph denoise [@lin2023text]

- [x] skim genie paper âœ… 2024-05-08

##### Diffusion-LM improves controllable text generation [@li2022diffusionlm]

- non-autoregressive (no more predicting the next token), instead: from Gaussian vectors into word vectors
- focus on *controllability*
	- **controllable text generation**: aims to generate $w$ that satisfies control target $c$, aka sample $w$ from $p(w \mid c)$

![[Pasted image 20240419225002.png]]

iterative gradient updates:
- Diffusion-LM optimizes for fluency; uses regularization
- additional query classifier makes sure control requirements are satisfied

adapting standard diffusion model for text:
- embedding function: discrete text -> continuous space
	- modify diffusion model training objective to jointly learn params and embeddings
- rounding: continuous vectors -> words
	- simple argmax doesn't work, thus reparameterization/clamping trick, aka forcing predicted vector to commit to a word for intermediate diffusion steps
# ChatGPT

>[!idea] idea: supervised training on dialogue + [[RLHF]]

**Cost model/Cost-to-go function**: optimize for cost/reward
-> good reward model: based on ranking (humans are bad at absolute ratings)

goal: #HHH - helpful, harmless, honest

1. humans demonstrate desired outputs -> fine-tune GPT-3.5
2. reward model based on comparison data
3. [PPO](https://en.wikipedia.org/wiki/Proximal_policy_optimization) reinforcement learning

[Blog post explaining more](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)
# Flash Attention

>[!idea] idea: load keys/queries/values in parallel

- still quadratic
- but makes better use of hardware
- [huggingface docs](https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention)
# From Models to Agents

>[!idea] idea: have LLMs interact with real world

humans are also just agents
# Notable Mentions

- GPT-4: best foundation model for now
- [BLOOM](https://huggingface.co/bigscience/bloom): open source, largest collab of AI researches
- [Galactica](https://galactica.org/static/paper.pdf): scientific LLM by Meta
- [LLama](https://huggingface.co/blog/llama3): open-source
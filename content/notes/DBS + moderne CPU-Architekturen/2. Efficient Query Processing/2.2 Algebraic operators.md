-> translate queries into relational algebra, often with multiple implementations (different usage scenarios)

# Table Scan

- usually already evaluates predicates
- navigates physical representation
- varying complexities: deferred updates, snapshot isolation (concurrency, secret tuples created by other transactions)

```cpp
TableScan::produce()
	for each page extent e
		for each page p in e
			fix p
			for each tuple t in p
			consumer.consume(t)
		unfix p
```

# Selection
often piggybacked

```cpp
Select::produce()
	input.produce()

Select::consume(t)
	if p(t)
		consumer.consume(t)
```

# Map
also often piggybacked

```cpp
Map::produce()
	input.produce()

Map::consume(t)
	t′ = t ◦ [a : f (t)]
	consumer.consume(t′)
```

- constant concatenation

# Join

- different implementations

depending on:
1. e2 depends on e1 (correlated subquery): nested loop
2. otherwise, if equi-join: any algorithm can be used
3. otherwise: nested loop or blockwise nested loop

### Nested-Loop join

- most flexible, but inefficient

>[!idea] evaluates rhs for every tuple on left side

```cpp
NLJoin::produce()
	left.produce()

NLJoin::consumeFromLeft(t)
	tL = t
	right.produce()

NLJoin::consumeFromRight(t)
	t′ = tL ◦ t
	if p(t′)
		consumer.consume(t′)
```

### Blockwise-Nested-Loop Join

>[!idea] loads as many tuples from lhs as possible -> reduce passes over rhs

- rhs evaluated once per memory size

```cpp
BNLJoin::produce()
	clear the tuple buffer
	left.produce()
	if tuple buffer is not empty
		right.produce()

BNLJoin::consumeFromLeft(t)
	if not can materialize t
		right.produce()
		clear the tuple buffer
	materialize t in tuple buffer

BNLJoin::consumeFromRight(t)
	for each tL in the tuple buffer
	t′ = tL ◦ t
	if p(t′)
		consumer.consumer(t′)
```

### (Sort-)Merge Join

>[!idea] assume join on $e_{1}.a = e_{2}.b$ with $e_{1}$ sorted on $a$ and $e_{2}$ sorted on $b$

- linear pass over input -> fast and efficient
- at least for 1:N

```cpp
SMJoin::produce()
	prepare a temp segment for spooling
	left.produce()
	IL =first spooled tuple
	right.produce()

SMJoin::consumeFromLeft(t)
	spool t to temp segment

SMJoin::consumeFromRight(t)
	while (∗IL).a < t.b
		advance IL
	IB = IL
	while (∗IB ).a = t.b
		consumer.consume((∗IB ) ◦ t)
		advance IB
```
(code materializes lhs for simpler code, usually not needed)

TODO

### Hash-Join

TODO

# Sort

TODO


# Group By

>[!int] easy way: sort first, fast way: hash-based aggregation

```cpp
InMemoryGroupBy::produce()
	initialize an empty hash table
	input.produce()
	for each group t in hash table
		consumer.consume(t)

InMemoryGroupBy::consume(t)
	if hash table[t|A] exists
		update the group hash table[t|A]
	else
		create a new group in hash table[t|A]
```

>[!imp] beware of variable-length aggregates
>- options: use max size, update sizes as we go, use references

# Set Operations

- `union`, `intersect`, `except` for set semantics
- `union all`, `intersect all`, `except all` for multiset semantics
- think: add, min, subtract frequencies
- can also exist with more than 2 inputs

>[!info] resemblance to join:
>

### `union all`

- concatenate tuple streams
- attribute rename

### `union`

- `all` + duplicate removal -> not usually done tho
- create one hash table for everyone to insert into
	- infrastructure needed anyway for other operations
	- pipeline breaker :(

### `intersect`

- if left side contains no duplicates: same as semi-join
- either check during build phase or, more commonly, use same strategy as `intersect all`

### `intersect all`

>[!int] intersection on a multiset:
>- minimum frequency
>- eg: { 1, 2, 2, 3, 3 } and { 2, 3, 3, 3, 4} result in { 2, 3, 3 }

- hash table group by: count occurrences on either side
- take minimum
- for more than 2 inputs: count 2 at a time + last input?

### `except`, `except all`

- refer to intersect
- subtract frequencies

# Window functions

```sql
sum(x) over (order by y)
```

-> can also specify a range

1. sort data
2. then aggregate -> how?

options:
- loop over range: $\mathcal{O}(n^2)$ bc $n$ queries for $n$ tuples
- sum until end - sum until begin: does not work for every aggregate (e.g. max)
- **segment tree**: $\mathcal{O}(n \log n)$ [WIkipedia](https://en.wikipedia.org/wiki/Segment_tree)
	- construct *for query*

```sql
count(x)
```

options:
- upper - lower, but what about `null` values?
- `sum(case when x is not null 1 else 0`

```sql
count(distinct x)
```

options:
- segment tree of sets: linear runtime for intersection :(
- remember next occurrence for each element:
	- number of unique values is number of values with next occurrence outside range
	- do this with segment tree to get lists for ranges
	- lookups can be done with binary search or: in constant time with **fractional cascading**
		- merged lists: remember position in original list (for some values, e.g. every 10)

```sql
count (distinct x) over (order by y)
```

- scan over tuples in y order, hash with x value as key
- hash table maps from x to pos